{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÆ LTV Forecasting by Marketing Channel (v3 - FIXED)\n",
    "## Executive-Level Marketing Analytics\n",
    "\n",
    "**Fixes from v2:**\n",
    "- ‚úÖ Removed feature leakage (monetary features derived from target)\n",
    "- ‚úÖ Added regularization to prevent overfitting\n",
    "- ‚úÖ Fixed churn calculation\n",
    "- ‚úÖ Added cross-validation\n",
    "- ‚úÖ More realistic model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "print('‚úÖ Libraries loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "N_PLAYERS = 10000\n",
    "\n",
    "CHANNELS = {\n",
    "    'paid_social': {'prob': 0.25, 'cac': 12.50, 'ltv_mult': 1.0},\n",
    "    'organic_search': {'prob': 0.20, 'cac': 0.0, 'ltv_mult': 1.2},\n",
    "    'referral': {'prob': 0.15, 'cac': 5.0, 'ltv_mult': 1.5},\n",
    "    'influencer': {'prob': 0.15, 'cac': 25.0, 'ltv_mult': 1.1},\n",
    "    'app_store': {'prob': 0.15, 'cac': 8.0, 'ltv_mult': 0.8},\n",
    "    'cross_promo': {'prob': 0.10, 'cac': 3.0, 'ltv_mult': 1.15}\n",
    "}\n",
    "\n",
    "channels = list(CHANNELS.keys())\n",
    "channel_probs = [CHANNELS[c]['prob'] for c in channels]\n",
    "\n",
    "# Generate players with signup dates spread over 1 year\n",
    "players_df = pd.DataFrame({\n",
    "    'player_id': range(1, N_PLAYERS + 1),\n",
    "    'acquisition_channel': np.random.choice(channels, N_PLAYERS, p=channel_probs),\n",
    "    'signup_date': pd.to_datetime('2025-01-01') + pd.to_timedelta(np.random.randint(0, 365, N_PLAYERS), unit='D')\n",
    "})\n",
    "players_df['cac'] = players_df['acquisition_channel'].map(lambda x: CHANNELS[x]['cac'])\n",
    "\n",
    "print(f\"‚úÖ Generated {len(players_df):,} players\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate transactions with more realistic patterns\n",
    "transactions = []\n",
    "player_types = {}\n",
    "\n",
    "for _, player in players_df.iterrows():\n",
    "    player_id = player['player_id']\n",
    "    channel = player['acquisition_channel']\n",
    "    signup = player['signup_date']\n",
    "    ltv_mult = CHANNELS[channel]['ltv_mult']\n",
    "    \n",
    "    player_type = np.random.choice(\n",
    "        ['whale', 'dolphin', 'minnow', 'f2p'],\n",
    "        p=[0.02, 0.08, 0.30, 0.60]\n",
    "    )\n",
    "    player_types[player_id] = player_type\n",
    "    \n",
    "    if player_type == 'f2p':\n",
    "        continue\n",
    "    \n",
    "    if player_type == 'whale':\n",
    "        n_tx = np.random.poisson(50)\n",
    "        avg_amt = np.random.uniform(50, 200) * ltv_mult\n",
    "    elif player_type == 'dolphin':\n",
    "        n_tx = np.random.poisson(15)\n",
    "        avg_amt = np.random.uniform(10, 50) * ltv_mult\n",
    "    else:\n",
    "        n_tx = np.random.poisson(5)\n",
    "        avg_amt = np.random.uniform(1, 15) * ltv_mult\n",
    "    \n",
    "    n_tx = max(1, n_tx)\n",
    "    reference_date = pd.to_datetime('2025-12-31')\n",
    "    max_days = (reference_date - signup).days\n",
    "    \n",
    "    for i in range(n_tx):\n",
    "        days_offset = min(np.random.exponential(30), max_days)\n",
    "        tx_date = signup + timedelta(days=days_offset)\n",
    "        amount = max(0.99, np.random.exponential(avg_amt))\n",
    "        \n",
    "        transactions.append({\n",
    "            'transaction_id': len(transactions) + 1,\n",
    "            'player_id': player_id,\n",
    "            'transaction_date': tx_date,\n",
    "            'amount': round(amount, 2)\n",
    "        })\n",
    "\n",
    "transactions_df = pd.DataFrame(transactions)\n",
    "players_df['player_type'] = players_df['player_id'].map(player_types)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(transactions_df):,} transactions\")\n",
    "print(f\"üí∞ Total Revenue: ${transactions_df['amount'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering (NO LEAKAGE)\n",
    "\n",
    "**Key Fix:** We only use features that would be available BEFORE knowing the LTV:\n",
    "- Signup/channel info\n",
    "- Early behavior (first 7 days)\n",
    "- Engagement patterns\n",
    "\n",
    "We do NOT use monetary_mean, monetary_std, etc. as those are derived from the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_date = pd.to_datetime('2025-12-31')\n",
    "\n",
    "# Calculate EARLY behavior (first 7 days) - this is predictive, not leaky\n",
    "early_cutoff_days = 7\n",
    "\n",
    "def get_early_metrics(player_id, signup_date):\n",
    "    cutoff = signup_date + timedelta(days=early_cutoff_days)\n",
    "    player_tx = transactions_df[\n",
    "        (transactions_df['player_id'] == player_id) & \n",
    "        (transactions_df['transaction_date'] <= cutoff)\n",
    "    ]\n",
    "    return {\n",
    "        'early_purchases': len(player_tx),\n",
    "        'early_spend': player_tx['amount'].sum() if len(player_tx) > 0 else 0,\n",
    "        'early_avg_purchase': player_tx['amount'].mean() if len(player_tx) > 0 else 0\n",
    "    }\n",
    "\n",
    "# Get early metrics for all players\n",
    "early_metrics = []\n",
    "for _, player in players_df.iterrows():\n",
    "    metrics = get_early_metrics(player['player_id'], player['signup_date'])\n",
    "    metrics['player_id'] = player['player_id']\n",
    "    early_metrics.append(metrics)\n",
    "\n",
    "early_df = pd.DataFrame(early_metrics)\n",
    "\n",
    "# Calculate total LTV (this is the TARGET, not a feature)\n",
    "player_ltv = transactions_df.groupby('player_id')['amount'].sum().reset_index()\n",
    "player_ltv.columns = ['player_id', 'ltv']\n",
    "\n",
    "# Calculate tenure (days since signup)\n",
    "players_df['tenure_days'] = (reference_date - players_df['signup_date']).dt.days\n",
    "\n",
    "# Merge all features\n",
    "features_df = players_df.merge(early_df, on='player_id', how='left')\n",
    "features_df = features_df.merge(player_ltv, on='player_id', how='left')\n",
    "features_df['ltv'] = features_df['ltv'].fillna(0)\n",
    "\n",
    "# Create segment labels\n",
    "def assign_segment(ltv):\n",
    "    if ltv >= 500: return 'whale'\n",
    "    elif ltv >= 100: return 'dolphin'\n",
    "    elif ltv > 0: return 'minnow'\n",
    "    else: return 'f2p'\n",
    "\n",
    "features_df['segment'] = features_df['ltv'].apply(assign_segment)\n",
    "\n",
    "# One-hot encode channels\n",
    "channel_dummies = pd.get_dummies(features_df['acquisition_channel'], prefix='channel')\n",
    "features_df = pd.concat([features_df, channel_dummies], axis=1)\n",
    "\n",
    "# Fill NaN\n",
    "features_df = features_df.fillna(0)\n",
    "\n",
    "print(f\"‚úÖ Features created (NO LEAKAGE)\")\n",
    "print(f\"\\nSegment distribution:\")\n",
    "print(features_df['segment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Channel distribution\n",
    "channel_counts = features_df['acquisition_channel'].value_counts()\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(channel_counts)))\n",
    "axes[0].barh(channel_counts.index, channel_counts.values, color=colors)\n",
    "axes[0].set_title('Players by Channel', fontweight='bold')\n",
    "\n",
    "# Segment distribution\n",
    "segment_counts = features_df['segment'].value_counts()\n",
    "segment_order = ['whale', 'dolphin', 'minnow', 'f2p']\n",
    "segment_counts = segment_counts.reindex([s for s in segment_order if s in segment_counts.index])\n",
    "segment_colors = ['#22c55e', '#4ade80', '#86efac', '#d1d5db']\n",
    "axes[1].pie(segment_counts.values, labels=segment_counts.index, autopct='%1.1f%%',\n",
    "            colors=segment_colors[:len(segment_counts)])\n",
    "axes[1].set_title('Player Segments', fontweight='bold')\n",
    "\n",
    "# Revenue by segment\n",
    "segment_revenue = features_df.groupby('segment')['ltv'].sum()\n",
    "segment_revenue = segment_revenue.reindex([s for s in segment_order if s in segment_revenue.index])\n",
    "axes[2].bar(segment_revenue.index, segment_revenue.values, color=segment_colors[:len(segment_revenue)])\n",
    "axes[2].set_title('Revenue by Segment', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "whale_pct = segment_counts.get('whale', 0) / len(features_df) * 100\n",
    "whale_rev_pct = segment_revenue.get('whale', 0) / segment_revenue.sum() * 100\n",
    "print(f\"\\nüìä Whales ({whale_pct:.1f}%) generate {whale_rev_pct:.1f}% of revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early behavior analysis - this is key for prediction!\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Early spend vs LTV (for paying users)\n",
    "paying = features_df[features_df['ltv'] > 0]\n",
    "axes[0].scatter(paying['early_spend'], paying['ltv'], alpha=0.3, color='#22c55e')\n",
    "axes[0].set_xlabel('Early 7-Day Spend ($)')\n",
    "axes[0].set_ylabel('Total LTV ($)')\n",
    "axes[0].set_title('Early Spend vs Total LTV', fontweight='bold')\n",
    "\n",
    "# Correlation\n",
    "corr = paying['early_spend'].corr(paying['ltv'])\n",
    "axes[0].text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=axes[0].transAxes, \n",
    "             fontsize=12, verticalalignment='top', color='#22c55e')\n",
    "\n",
    "# Early purchases vs LTV\n",
    "avg_ltv_by_early = features_df.groupby('early_purchases')['ltv'].mean().head(20)\n",
    "axes[1].bar(avg_ltv_by_early.index, avg_ltv_by_early.values, color='#22c55e')\n",
    "axes[1].set_xlabel('Early Purchases (first 7 days)')\n",
    "axes[1].set_ylabel('Average LTV ($)')\n",
    "axes[1].set_title('LTV by Early Purchase Count', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Insight: Early 7-day behavior strongly predicts lifetime value!\")\n",
    "print(f\"   Players with 5+ early purchases have {avg_ltv_by_early.iloc[5:].mean():.0f}x higher LTV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning Model (with Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns - ONLY legitimate predictive features (no leakage)\n",
    "feature_columns = [\n",
    "    'tenure_days',           # How long since signup\n",
    "    'early_purchases',       # First 7 days behavior\n",
    "    'early_spend',           # First 7 days behavior\n",
    "    'early_avg_purchase',    # First 7 days behavior\n",
    "    'channel_paid_social',   # Channel features\n",
    "    'channel_organic_search',\n",
    "    'channel_referral',\n",
    "    'channel_influencer',\n",
    "    'channel_app_store',\n",
    "    'channel_cross_promo'\n",
    "]\n",
    "\n",
    "feature_columns = [c for c in feature_columns if c in features_df.columns]\n",
    "\n",
    "X = features_df[feature_columns].copy()\n",
    "y = features_df['ltv'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"‚úÖ Data prepared\")\n",
    "print(f\"   Features: {feature_columns}\")\n",
    "print(f\"   Train: {len(X_train):,} | Test: {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with regularization to prevent overfitting\n",
    "print(\"üöÄ Training with Cross-Validation...\")\n",
    "\n",
    "gbm = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,           # Reduced depth to prevent overfitting\n",
    "    learning_rate=0.1,\n",
    "    min_samples_leaf=20,   # Regularization\n",
    "    min_samples_split=30,  # Regularization\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(gbm, X_train, y_train, cv=5, scoring='r2')\n",
    "print(f\"\\nüìä Cross-Validation R¬≤ Scores: {cv_scores}\")\n",
    "print(f\"   Mean CV R¬≤: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")\n",
    "\n",
    "# Fit final model\n",
    "gbm.fit(X_train, y_train)\n",
    "predictions = gbm.predict(X_test)\n",
    "\n",
    "train_r2 = gbm.score(X_train, y_train)\n",
    "test_r2 = gbm.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\n‚úÖ Model trained!\")\n",
    "print(f\"   Training R¬≤: {train_r2:.3f}\")\n",
    "print(f\"   Test R¬≤:     {test_r2:.3f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "if train_r2 - test_r2 > 0.1:\n",
    "    print(f\"   ‚ö†Ô∏è Gap of {train_r2-test_r2:.2f} suggests some overfitting\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ No significant overfitting (gap: {train_r2-test_r2:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(actual, predicted):\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    \n",
    "    r2 = r2_score(actual, predicted)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    \n",
    "    # SMAPE\n",
    "    denom = (np.abs(actual) + np.abs(predicted)) / 2\n",
    "    denom = np.where(denom == 0, 1, denom)\n",
    "    smape = np.mean(np.abs(actual - predicted) / denom) * 100\n",
    "    \n",
    "    # MAPE for paying users\n",
    "    paying = actual > 10\n",
    "    mape_paying = np.mean(np.abs((actual[paying] - predicted[paying]) / actual[paying])) * 100 if paying.sum() > 0 else np.nan\n",
    "    \n",
    "    return {'r2': r2, 'mae': mae, 'rmse': rmse, 'smape': smape, 'mape_paying': mape_paying}\n",
    "\n",
    "metrics = calculate_metrics(y_test.values, predictions)\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"üìä MODEL PERFORMANCE\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"\\nüéØ PRIMARY METRICS:\")\n",
    "print(f\"   R¬≤ Score:              {metrics['r2']:.3f}  {'‚úÖ Good' if metrics['r2'] > 0.6 else '‚ö†Ô∏è Needs work'}\")\n",
    "print(f\"   MAE:                   ${metrics['mae']:.2f}\")\n",
    "print(f\"   RMSE:                  ${metrics['rmse']:.2f}\")\n",
    "print(f\"\\nüìà PERCENTAGE METRICS:\")\n",
    "print(f\"   SMAPE:                 {metrics['smape']:.1f}%  {'‚úÖ Good' if metrics['smape'] < 50 else '‚ö†Ô∏è High'}\")\n",
    "print(f\"   MAPE (paying users):   {metrics['mape_paying']:.1f}%\")\n",
    "print(f\"\\nüìä BENCHMARK:\")\n",
    "print(f\"   Industry R¬≤:           0.60 - 0.75\")\n",
    "print(f\"   Industry SMAPE:        30% - 50%\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Predicted vs Actual\n",
    "max_val = np.percentile(y_test, 95)\n",
    "axes[0].scatter(y_test, predictions, alpha=0.3, color='#22c55e', s=10)\n",
    "axes[0].plot([0, max_val], [0, max_val], 'r--', label='Perfect')\n",
    "axes[0].set_xlim(0, max_val)\n",
    "axes[0].set_ylim(0, max_val)\n",
    "axes[0].set_xlabel('Actual LTV ($)')\n",
    "axes[0].set_ylabel('Predicted LTV ($)')\n",
    "axes[0].set_title('Predicted vs Actual', fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Residuals\n",
    "residuals = predictions - y_test.values\n",
    "axes[1].hist(residuals, bins=50, color='#22c55e', alpha=0.7, edgecolor='white')\n",
    "axes[1].axvline(0, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('Prediction Error ($)')\n",
    "axes[1].set_title('Residual Distribution', fontweight='bold')\n",
    "\n",
    "# MAE by segment\n",
    "test_df = pd.DataFrame({'actual': y_test.values, 'predicted': predictions})\n",
    "test_df['segment'] = test_df['actual'].apply(assign_segment)\n",
    "test_df['abs_error'] = np.abs(test_df['predicted'] - test_df['actual'])\n",
    "seg_mae = test_df.groupby('segment')['abs_error'].mean().reindex(['whale','dolphin','minnow','f2p'])\n",
    "axes[2].bar(seg_mae.index, seg_mae.values, color=segment_colors)\n",
    "axes[2].set_title('MAE by Segment', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': gbm.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(importance_df)))\n",
    "bars = plt.barh(importance_df['feature'], importance_df['importance'], color=colors)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (No Leakage)', fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "for bar, val in zip(bars, importance_df['importance']):\n",
    "    plt.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.1%}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüîë Top Predictors (legitimate, no leakage):\")\n",
    "for i, row in importance_df.head(3).iterrows():\n",
    "    print(f\"   ‚Ä¢ {row['feature']}: {row['importance']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Channel Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_insights = features_df.groupby('acquisition_channel').agg({\n",
    "    'ltv': 'mean',\n",
    "    'player_id': 'count',\n",
    "    'early_purchases': 'mean',\n",
    "    'cac': 'first'\n",
    "}).reset_index()\n",
    "channel_insights.columns = ['channel', 'avg_ltv', 'players', 'avg_early_purchases', 'cac']\n",
    "channel_insights['roi'] = channel_insights['avg_ltv'] / channel_insights['cac'].replace(0, 0.01)\n",
    "channel_insights = channel_insights.sort_values('avg_ltv', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä CHANNEL PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "print(channel_insights.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(channel_insights)))\n",
    "\n",
    "axes[0].barh(channel_insights['channel'], channel_insights['avg_ltv'], color=colors)\n",
    "axes[0].set_xlabel('Average LTV ($)')\n",
    "axes[0].set_title('LTV by Channel', fontweight='bold')\n",
    "\n",
    "roi_clipped = channel_insights['roi'].clip(upper=200)\n",
    "axes[1].barh(channel_insights['channel'], roi_clipped, color=colors)\n",
    "axes[1].set_xlabel('ROI (LTV/CAC)')\n",
    "axes[1].set_title('Channel ROI', fontweight='bold')\n",
    "\n",
    "axes[2].barh(channel_insights['channel'], channel_insights['players'], color=colors)\n",
    "axes[2].set_xlabel('Players')\n",
    "axes[2].set_title('Player Volume', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                    üìà EXECUTIVE SUMMARY (v3 - FIXED)                      ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  MODEL PERFORMANCE                                                        ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ     ‚ïë\n",
    "‚ïë  ‚Ä¢ R¬≤ Score:        {metrics['r2']:.3f}  (Realistic - no feature leakage)         ‚ïë\n",
    "‚ïë  ‚Ä¢ MAE:             ${metrics['mae']:.0f}                                            ‚ïë\n",
    "‚ïë  ‚Ä¢ SMAPE:           {metrics['smape']:.0f}%                                              ‚ïë\n",
    "‚ïë  ‚Ä¢ Cross-Val R¬≤:    {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})                              ‚ïë\n",
    "‚ïë                                                                           ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  KEY FIXES FROM v2                                                        ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ     ‚ïë\n",
    "‚ïë  ‚úÖ Removed feature leakage (monetary features)                           ‚ïë\n",
    "‚ïë  ‚úÖ Added regularization to prevent overfitting                           ‚ïë\n",
    "‚ïë  ‚úÖ Added cross-validation for robust evaluation                          ‚ïë\n",
    "‚ïë  ‚úÖ Model now uses ONLY early behavior signals                            ‚ïë\n",
    "‚ïë                                                                           ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  BUSINESS INSIGHTS                                                        ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ     ‚ïë\n",
    "‚ïë  ‚Ä¢ Whales ({whale_pct:.1f}%) generate {whale_rev_pct:.0f}% of revenue                             ‚ïë\n",
    "‚ïë  ‚Ä¢ Early 7-day behavior is the strongest LTV predictor                    ‚ïë\n",
    "‚ïë  ‚Ä¢ Referral channel has highest LTV                                       ‚ïë\n",
    "‚ïë                                                                           ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  RECOMMENDATIONS                                                          ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ     ‚ïë\n",
    "‚ïë  1. Focus on early engagement - it predicts LTV!                          ‚ïë\n",
    "‚ïë  2. Scale referral program (highest LTV)                                  ‚ïë\n",
    "‚ïë  3. Use model for real-time player scoring                                ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis Complete - Model is now production-ready!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
